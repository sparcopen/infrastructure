---
layout: post
title: Strategic Choices
category: roadmap-for-action
tags: 
description: The second category of actions is more complex, since it relates to decisions that will need to be made specifically based on each individual institution’s mission, culture and values. It also involves the establishment of an explicit process to determine the position that each institution wants to take in regards to specific issues posed by the collection of data and the deployment of data analytics tools.
permalink: /roadmap-for-action/strategic-choices
date: 2019-11-01 07:00:00
---

The second category of actions is more complex, since it relates to decisions that will need to be made specifically based on each individual institution’s mission, culture and values. It also involves the establishment of an explicit process to determine the position that each institution wants to take in regards to specific issues posed by the collection of data and the deployment of data analytics tools.

SPARC’s goal is not to provide answers in this section. Rather, we hope to trigger a broad and thoughtful debate within academic institutions to ensure that these processes are explicitly carried out. The criteria for decision-making in this category are more complex than in the case of risk mitigation actions. It is hard to argue, for example, that the institution should not have a strong privacy policy or conduct a data inventory. The choice between competing actions simply comes down to what is feasible within the institution’s resources, culture, and timeline. On the other hand, this section deals with choices that do not have clear right or wrong answers, and where there will need to be a nuanced debate.

It is vital that these debates involve all stakeholders on campus. These debates will be complex and multifaceted, with ethical, legal, economic, and technical dimensions. Many institutions will have scholars and practitioners in these fields right on campus and would be wise to leverage this expertise in structuring the debate. This is very important because there will be diverging views on what the right answers are, but decisions will be more acceptable if reached after using a well-structured approach.

### Algorithms vs. Humans

The debate over using artificial intelligence as a substitute for human analysis is already playing out in many parts of society, including the corporate sector. As an example, Forbes recently developed a list of 15 business applications of artificial intelligence.[^9] Out of these 15 applications, at least five apply to academic institutions as well. Should institutions deploy artificial intelligence tools in the admission process? In recruiting staff? In reviewing and grading non-quantitative exam materials? In identifying potential malicious or unsafe student behavior before it occurs? Should students be able to use accelerated reading software? Should software provide a first line of student support, substituting for teaching assistants? The answer to each of these questions is complex and will vary across and within types of institutions.

One of the early use cases for algorithms on campus was plagiarism checking software such as Turnitin, which has recently sparked debates over accuracy, accountability, and bias.[^10] Books such as Weapons of Math Destruction and Algorithms of Oppression have highlighted how algorithms can perpetuate inequities through built-in biases and negative feedback loops. As such, there are significant ethical and legal implications of using algorithms to drive decision-making.

However, there also may be implications of not using them. Algorithms can process information more rapidly than humans and provide tailored services to students (such as adaptive learning) that would be cost prohibitive to deliver through faculty or staff. Also, while algorithms will inevitably contain biases that are built in from the start, machines can analyze datasets more consistently and efficiently than individual humans ever could.

Either way, it is only a matter of time before artificial intelligence further pervades campus decision-making in ways that impact equity, privacy, and allocation of resources. Academic senates, institutional governance boards, and other decision- making bodies should begin a dialogue over the pros and cons as soon as possible. Engaging in this debate in advance will help prepare institutions to be deliberate and strategic about deploying artificial intelligence in ways that are consistent with the institution’s culture, values, and risk tolerance.

### Quantitative vs. Qualitative Metrics

A second critical set of debates that are necessary is around the metrics that academic institutions use for evaluation. This debate often focuses on the issue of faculty evaluation. Most universities argue that they promote their faculty at all levels through a thorough process of evaluation of each individual, looking both at their intellectual achievements and at their personal contributions to teaching and the life of the institution. Nonetheless, a recurrent complaint is the overbearing impact of publications records and journal impact factors (at least in the disciplines and institutions in which this metric is relevant).

Some institutions are already critically reevaluating how they use quantitative metrics. The University of Ghent in Belgium announced in December 2018 that it would change how it evaluates its faculty. In the announcement, Rector Rik Van de Walle wrote:
>“No more procedures and processes with always the same templates, metrics and criteria which lump everyone together” and “The model must provide a response to the complaint of many young professors that quantitative parameters are predominant in the evaluation process. The well-known and overwhelming 'publication pressure' is the most prominent exponent of this. Ghent University is deliberately choosing to step out of the rat race between individuals, departments and universities. We no longer wish to participate in the ranking of people”.

More broadly, the the San Francisco Declaration on Research Assessment (DORA)[^11] and the Leiden manifesto[^12] provide additional valuable frameworks on how to think about research assessment and should be viewed as a valuable starting point on how to transform and enrich the assessment process of faculty and researchers. It may be necessary to also define appropriate metrics to address the complexities of interdisciplinary work, which often are not recognized through traditional metrics.

The debate over metrics also extends to evaluation in other areas of campus life, including academic programs, grading, and return on investment for campus programs. While academic institutions may not be ready to altogether abandon the usage of quantitative metrics to evaluate their faculty, they should consider engaging in a genuine debate on the relative weight that they place on quantitative vs. qualitative assessment, and whether the quantitative metrics they use are representative of the objectives of the institution or just happen to be convenient because they are easily available and easily comparable.

### IP Exploitation vs. Knowledge Sharing

Many academic institutions house valuable intellectual property (IP) that is generated through the research activity of its community. While U.S. institutions have been allowed to pursue ownership of inventions based on federally-funded research since passage of the Bayh-Dole Act in 1980, few research universities have successfully reaped rewards, despite the enormous potential value.[^13]

The emergence of “big data” and text and data mining has opened up new possibilities for research universities to exploit their IP in profitable ways. Articles and datasets can be mined for insights that can be used by industry, for example to improve the odds of profitable investments in R&D or venture capital. Such activities could generate substantial value for academic research institutions, particularly at a time when the future of government funding is clouded by budget constraints and international competition among academic institutions is rising, driving the need for larger budgets.

On the other hand, vigorous IP exploitation would likely raise a number of ethical issues around partnering with specific industries and companies, as well as concerns that prioritizing IP exploitation could shift resources away from disciplines with less commercial value. Moreover, any decision to exploit data and knowledge for commercial and financial purposes must be weighed against the benefits of Open Data for accelerating the pace of discovery and increasing the integrity of the scientific and scholarly record.

This debate is not necessarily mutually exclusive. For example, it may be possible to maintain an Open Data policy for baseline-quality datasets, while setting up a second flow for datasets that have been processed, cleaned, and standardized for IP exploitation. This structure could provide a “best of both worlds” scenario, where grant funding could support the first flow and commercial services could pay for access to the second flow.

While SPARC is known for advocating for Open Data when possible, we recognize that different institutions can legitimately adjudicate this issue differently. Our goal in this document is not to prescribe answers, but to encourage institutions to hold a broad and thoughtful debate to decide this issue for themselves.


***
[^9]: [https://www.forbes.com/sites/forbestechcouncil/2018/09/27/15-business-applications-for-artificial-intelligence-and-machine-learning/#4d87bff3579f](https://www.forbes.com/sites/forbestechcouncil/2018/09/27/15-business-applications-for-artificial-intelligence-and-machine-learning/#4d87bff3579f)

[^10]: [https://www.insidehighered.com/news/2017/06/19/anti-turnitin-manifesto-calls-resistance-some-technology-digital-age](https://www.insidehighered.com/news/2017/06/19/anti-turnitin-manifesto-calls-resistance-some-technology-digital-age)

[^11]: [https://sfdora.org/](https://sfdora.org/)

[^12]: [http://www.leidenmanifesto.org/](http://www.leidenmanifesto.org/)

[^13]:  In 2012, the then President of the Association of University Technology Managers testified to congress that as much as 30% of the market capitalization of NASDAQ was driven by academic research. [https://www.govinfo.gov/content/pkg/CHRG-112hhrg74722/html/CHRG-112hhrg74722.htm](https://www.govinfo.gov/content/pkg/CHRG-112hhrg74722/html/CHRG-112hhrg74722.htm)
